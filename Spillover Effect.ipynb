{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'BTC.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Đọc dữ liệu từ các tệp đã cung cấp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m btc_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBTC.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m      7\u001b[0m gold_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGOLD.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m vni_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVNI.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tri Duc\\miniconda3\\envs\\python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tri Duc\\miniconda3\\envs\\python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Tri Duc\\miniconda3\\envs\\python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tri Duc\\miniconda3\\envs\\python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Tri Duc\\miniconda3\\envs\\python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'BTC.csv'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Đọc dữ liệu từ các tệp đã cung cấp\n",
    "btc_data = pd.read_csv('BTC.csv')  \n",
    "gold_data = pd.read_csv('GOLD.csv')\n",
    "vni_data = pd.read_csv('VNI.csv')\n",
    "\n",
    "# Chuyển đổi cột ngày về dạng datetime\n",
    "btc_data['Date'] = pd.to_datetime(btc_data['Date'])\n",
    "gold_data['Date'] = pd.to_datetime(gold_data['Date'])\n",
    "vni_data['Date'] = pd.to_datetime(vni_data['Date'])\n",
    "\n",
    "btc_data['Cchange'] = pd.to_numeric(btc_data['Cchange'].str.rstrip('%').replace('#DIV/0!', np.nan), errors='coerce') / 100\n",
    "vni_data['Cchange'] = pd.to_numeric(vni_data['Cchange'].str.rstrip('%').replace('#DIV/0!', np.nan), errors='coerce') / 100\n",
    "\n",
    "\n",
    "gold_data = gold_data[(gold_data['Date'] >= '2021-01-01') & (gold_data['Date'] <= '2024-09-01')]\n",
    "btc_data = btc_data[(btc_data['Date'] >= '2021-01-01') & (btc_data['Date'] <= '2024-09-01')]\n",
    "vni_data = vni_data[(vni_data['Date'] >= '2021-01-01') & (vni_data['Date'] <= '2024-09-01')]\n",
    "# Tạo figure và subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 10))\n",
    "\n",
    "# Vẽ biểu đồ cho Bitcoin (BTC)\n",
    "axs[0].plot(btc_data['Date'], btc_data[\"Cchange\"], color='gray')\n",
    "axs[0].set_title('BTC')\n",
    "axs[0].set_ylabel('Daily Return')\n",
    "\n",
    "# Vẽ biểu đồ cho Gold\n",
    "axs[1].plot(gold_data['Date'], gold_data[\"Vola\"], color='gray')\n",
    "axs[1].set_title('Vàng')\n",
    "axs[1].set_ylabel('Daily Return')\n",
    "\n",
    "# Vẽ biểu đồ cho VNI\n",
    "axs[2].plot(vni_data['Date'], vni_data[\"Cchange\"], color='gray')\n",
    "axs[2].set_title('VNI')\n",
    "axs[2].set_ylabel('Daily Return')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_data_filtered = btc_data.loc[(btc_data['Date'] >= '2021-01-01') & (btc_data['Date'] <= '2024-09-01')][::-1]\n",
    "vni_data_filtered = vni_data.loc[(vni_data['Date'] >= '2021-01-01') & (vni_data['Date'] <= '2024-09-01')]\n",
    "gold_data_filtered = gold_data.loc[(gold_data['Date'] >= '2021-01-01') & (gold_data['Date'] <= '2024-09-01')]\n",
    "\n",
    "# Chuyển đổi cột \"Cchange\" về kiểu số, bỏ đi ký tự %\n",
    "btc_data_filtered['Cchange'] = pd.to_numeric(btc_data_filtered['Cchange'], errors='coerce')\n",
    "vni_data_filtered['Cchange'] = pd.to_numeric(vni_data_filtered['Cchange'], errors='coerce')\n",
    "gold_data_filtered['Vola'] = pd.to_numeric(gold_data_filtered['Vola'], errors='coerce')\n",
    "# Loại bỏ các giá trị NaN\n",
    "btc_returns = btc_data_filtered['Cchange'].dropna()\n",
    "vnireturns = vni_data_filtered['Cchange'].dropna()\n",
    "gold_returns = gold_data_filtered['Vola'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "\n",
    "# Đổi tên Series\n",
    "btc_new = btc_returns.rename(\"BTC\")\n",
    "gold_new = gold_returns.rename(\"Gold\")\n",
    "vninew = vnireturns.rename(\"VNI\")\n",
    "\n",
    "#reset index cho từng dữ liệu\n",
    "btc_new = btc_new.reset_index(drop=True)\n",
    "vninew= vninew.reset_index(drop=True)\n",
    "gold_new= gold_new.reset_index(drop=True)\n",
    "\n",
    "# Kết hợp dữ liệu (merge)\n",
    "merged_data = pd.concat([btc_new, gold_new, vninew], axis=1)\n",
    "\n",
    "# Tính toán ma trận tương quan Kendall và Spearman\n",
    "kendall_corr = merged_data.corr(method='kendall')\n",
    "spearman_corr = merged_data.corr(method='spearman')\n",
    "\n",
    "# Vẽ heatmap cho Kendall\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(kendall_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Kendall Rank Correlation')\n",
    "\n",
    "# Vẽ heatmap cho Spearman\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Spearman Rank Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis, jarque_bera\n",
    "\n",
    "data = merged_data\n",
    "# Tạo một DataFrame để lưu kết quả\n",
    "results = pd.DataFrame(columns=['Mean', 'S.D.', 'Skewness', 'Kurtosis', 'Jarque-Bera', 'Jarque-Bera p-value'])\n",
    "\n",
    "# Tính các chỉ số thống kê cho mỗi tài sản\n",
    "for asset in ['Gold', 'VNI', 'BTC']:  \n",
    "    mean = data[asset].mean()\n",
    "    std_dev = data[asset].std()\n",
    "    skewness = skew(data[asset])\n",
    "    kurt = kurtosis(data[asset])\n",
    "    jb_stat, jb_pvalue = jarque_bera(data[asset])\n",
    "    \n",
    "    results.loc[asset] = [mean, std_dev, skewness, kurt, jb_stat, jb_pvalue]\n",
    "\n",
    "print(\"Descriptive Statistics for Gold, VNI, and BTC:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch\n",
    "\n",
    "def ljung_box_test(series, lags=20):\n",
    "    result = acorr_ljungbox(series, lags=[lags], return_df=True)\n",
    "    return {\n",
    "        \"Ljung-Box Q Statistic\": result.iloc[0, 0],\n",
    "        \"p-value\": result.iloc[0, 1],\n",
    "        \"No Autocorrelation\": result.iloc[0, 1] > 0.05  \n",
    "    }\n",
    "\n",
    "def arch_lm_test(series):\n",
    "    result = het_arch(series)\n",
    "    return {\n",
    "        \"ARCH-LM Statistic\": result[0],\n",
    "        \"p-value\": result[1],\n",
    "        \"No ARCH Effect\": result[1] > 0.05  \n",
    "    }\n",
    "\n",
    "\n",
    "ljung_box_results = []\n",
    "arch_lm_results = []\n",
    "\n",
    "for column in merged_data.columns:\n",
    "    series = merged_data[column].dropna()\n",
    "    ljung_box_results.append({\"Column\": column, **ljung_box_test(series)})\n",
    "    arch_lm_results.append({\"Column\": column, **arch_lm_test(series)})\n",
    "\n",
    "\n",
    "ljung_box_results_df = pd.DataFrame(ljung_box_results)\n",
    "arch_lm_results_df = pd.DataFrame(arch_lm_results)\n",
    "\n",
    "print(ljung_box_results_df)\n",
    "print(arch_lm_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import kpss, adfuller\n",
    "\n",
    "data=merged_data\n",
    "\n",
    "def kpss_test(series, column_name):\n",
    "    result = kpss(series, regression='c', nlags=\"auto\")\n",
    "    output = {\n",
    "        \"Column\": column_name,\n",
    "        \"KPSS Statistic\": result[0],\n",
    "        \"p-value\": result[1],\n",
    "        \"Critical Values\": result[3],\n",
    "        \"Stationary\": result[1] > 0.05  \n",
    "    }\n",
    "    return output\n",
    "\n",
    "\n",
    "kpss_results = []\n",
    "for column in data.columns:\n",
    "    kpss_results.append(kpss_test(data[column].dropna(), column))\n",
    "\n",
    "kpss_results_df = pd.DataFrame(kpss_results)\n",
    "\n",
    "def adf_test(series, column_name):\n",
    "    result = adfuller(series, autolag='AIC')\n",
    "    output = {\n",
    "        \"Column\": column_name,\n",
    "        \"ADF Statistic\": result[0],\n",
    "        \"p-value\": result[1],\n",
    "        \"Critical Values\": result[4],\n",
    "        \"Stationary\": result[1] < 0.05  \n",
    "    }\n",
    "    return output\n",
    "\n",
    "results = []\n",
    "for column in data.columns:\n",
    "    results.append(adf_test(data[column].dropna(), column))\n",
    "\n",
    "adf_results = pd.DataFrame(results)\n",
    "\n",
    "print(kpss_results_df)\n",
    "print(adf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Q-Q Plots for Cchange and Vola Returns')\n",
    "\n",
    "# Q-Q plot for BTC \n",
    "stats.probplot(btc_returns, dist=\"norm\", plot=axes[0])\n",
    "axes[0].set_title('BTC Returns')\n",
    "\n",
    "# Q-Q plot for GOLD \n",
    "stats.probplot(gold_returns, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('GOLD Returns')\n",
    "\n",
    "# Q-Q plot for VNI \n",
    "stats.probplot(vnireturns, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('VNI Returns')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "def plot_acf_for_multiple_returns(returns_list, titles):\n",
    "    fig, axes = plt.subplots(1, len(returns_list), figsize=(15, 5), sharey=True)\n",
    "    for i, (returns, title) in enumerate(zip(returns_list, titles)):\n",
    "        plot_acf(returns, lags=40, alpha=0.05, ax=axes[i])\n",
    "        axes[i].set_title(f'ACF ({title}) Returns')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "returns_list = [btc_returns, gold_returns, vnireturns]\n",
    "titles = ['BTC', 'GOLD', 'VNI']\n",
    "\n",
    "plot_acf_for_multiple_returns(returns_list, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from arch import arch_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fit_gjr_garch_and_get_residuals(returns, title):\n",
    "\n",
    "    model = arch_model(returns, vol='Garch', p=1, o=1, q=1, dist='Normal')\n",
    "    fitted_model = model.fit(disp='off')\n",
    "    \n",
    "    # Lấy phần dư\n",
    "    residuals = fitted_model.resid\n",
    "    \n",
    "    print(f\"Summary for {title} GJR-GARCH Model:\")\n",
    "    print(fitted_model.summary())\n",
    "    \n",
    "    return residuals\n",
    "\n",
    "btc_residuals = fit_gjr_garch_and_get_residuals(btc_returns, 'BTC')\n",
    "gold_residuals = fit_gjr_garch_and_get_residuals(gold_returns, 'GOLD')\n",
    "vni_residuals = fit_gjr_garch_and_get_residuals(vnireturns, 'VNI')\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(btc_residuals)\n",
    "plt.title('Residuals for BTC Returns')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(gold_residuals)\n",
    "plt.title('Residuals for GOLD Returns')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(vni_residuals)\n",
    "plt.title('Residuals for VNI Returns')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "sns.histplot(btc_residuals, kde=True, stat=\"density\", label=\"BTC Residuals\")\n",
    "sns.histplot(gold_residuals, kde=True, stat=\"density\", label=\"GOLD Residuals\", color='orange')\n",
    "sns.histplot(vni_residuals, kde=True, stat=\"density\", label=\"VNI Residuals\", color='green')\n",
    "plt.legend()\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Chuẩn hóa phần dư\n",
    "def normalize_residuals(residuals):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(residuals.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "btc_residuals_norm = normalize_residuals(btc_residuals)\n",
    "gold_residuals_norm = normalize_residuals(gold_residuals)\n",
    "vni_residuals_norm = normalize_residuals(vni_residuals)\n",
    "\n",
    "residuals_df = pd.DataFrame({\n",
    "    'BTC': btc_residuals_norm,\n",
    "    'GOLD': gold_residuals_norm,\n",
    "    'VNI': vni_residuals_norm\n",
    "})\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vẽ ma trận scatter plot\n",
    "sns.pairplot(\n",
    "    residuals_df, \n",
    "    diag_kind='kde',  \n",
    "    plot_kws={'alpha': 0.6, 's': 15}  \n",
    ")\n",
    "\n",
    "plt.suptitle('Scatter Plot Matrix of GJR-GARCH Residuals', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import genpareto, norm, chi2\n",
    "import numpy as np\n",
    "from copulas.bivariate import Clayton, Frank\n",
    "\n",
    "def compute_gng_and_uniform(residuals, title=\"\"):\n",
    "    \n",
    "    lower_threshold = np.quantile(residuals, 0.05)  \n",
    "    upper_threshold = np.quantile(residuals, 0.95)  \n",
    "\n",
    "   \n",
    "    left_tail = residuals[residuals < lower_threshold]\n",
    "    body = residuals[(residuals >= lower_threshold) & (residuals <= upper_threshold)]\n",
    "    right_tail = residuals[residuals > upper_threshold]\n",
    "    \n",
    "\n",
    "    left_params = genpareto.fit(-left_tail)  \n",
    "    right_params = genpareto.fit(right_tail)\n",
    "\n",
    "\n",
    "    def gng_cdf(x):\n",
    "        if x < lower_threshold:\n",
    "            return genpareto.cdf(-x, *left_params) * 0.05\n",
    "        elif x > upper_threshold:\n",
    "            return 0.95 + genpareto.cdf(x - upper_threshold, *right_params) * 0.05\n",
    "        else:\n",
    "            return 0.05 + norm.cdf(x, loc=body.mean(), scale=body.std()) * 0.9\n",
    "\n",
    "    cdf_vectorized = np.vectorize(gng_cdf)\n",
    "    uniform_data = cdf_vectorized(residuals)\n",
    "\n",
    "\n",
    "    return uniform_data\n",
    "\n",
    "# Tính toán theo phân phối GNG và uniform đối với phần dư của mô hình GARCH BTC, GOLD và VNI\n",
    "btc_uniform_data = compute_gng_and_uniform(btc_residuals, title='BTC')\n",
    "gold_uniform_data = compute_gng_and_uniform(gold_residuals, title='GOLD')\n",
    "vni_uniform_data = compute_gng_and_uniform(vni_residuals, title='VNI')\n",
    "\n",
    "def compute_log_likelihood_and_p_value(copula, data):\n",
    "    try:\n",
    "        # Tính log-likelihood của mô hình fitted\n",
    "        pdf = copula.probability_density(data)\n",
    "        log_likelihood = np.sum(np.log(pdf))\n",
    "\n",
    "        # Tính log-likelihood của mô hình độc lập (independence copula)\n",
    "        independent_log_likelihood = np.sum(np.log(data[:, 0]) + np.log(data[:, 1]))\n",
    "\n",
    "        # Tính giá trị kiểm định Likelihood Ratio Test (LRT)\n",
    "        likelihood_ratio_stat = 2 * (log_likelihood - independent_log_likelihood)\n",
    "        p_value = 1 - chi2.cdf(likelihood_ratio_stat, df=1)  # df = 1 do chỉ có 1 tham số\n",
    "\n",
    "        return log_likelihood, likelihood_ratio_stat, p_value\n",
    "    except Exception as e:\n",
    "        print(f\"Error during computation: {e}\")\n",
    "        return \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "urc_data = pd.DataFrame({\n",
    "    'BTC': btc_uniform_data,\n",
    "    'GOLD': gold_uniform_data,\n",
    "    'VNI': vni_uniform_data\n",
    "})\n",
    "\n",
    "# Lấy tên các cột để phân tích từng cặp đối tượng\n",
    "columns = urc_data.columns\n",
    "num_columns = len(columns)\n",
    "\n",
    "# Lưu trữ kết quả tính toán\n",
    "results = []\n",
    "\n",
    "# Lặp qua từng cặp cột để phân tích bằng các mô hình Copula\n",
    "for i in range(num_columns):\n",
    "    for j in range(i + 1, num_columns):\n",
    "        col1 = columns[i]\n",
    "        col2 = columns[j]\n",
    "        \n",
    "        # Chuẩn bị dữ liệu của cặp hiện tại\n",
    "        data1 = urc_data[col1]\n",
    "        data2 = urc_data[col2]\n",
    "        copula_data_np = np.column_stack((data1, data2))\n",
    "\n",
    "        # Phân tích bằng Frank Copula\n",
    "        frank_copula = Frank()\n",
    "        try:\n",
    "            frank_copula.fit(copula_data_np)\n",
    "            frank_theta = frank_copula.theta\n",
    "            frank_log_likelihood, frank_lrt_stat, frank_p_value = compute_log_likelihood_and_p_value(frank_copula, copula_data_np)\n",
    "            frank_result = f\"Theta: {frank_theta:.4f}, Log-Lik: {frank_log_likelihood:.4f}, LRT: {frank_lrt_stat:.4f}, p: {frank_p_value:.4f}\"\n",
    "        except Exception as e:\n",
    "            frank_result = f\"Error: {e}\"\n",
    "\n",
    "        # Phân tích bằng Clayton Copula\n",
    "        clayton_copula = Clayton()\n",
    "        try:\n",
    "            clayton_copula.fit(copula_data_np)\n",
    "            clayton_theta = clayton_copula.theta\n",
    "            clayton_log_likelihood, clayton_lrt_stat, clayton_p_value = compute_log_likelihood_and_p_value(clayton_copula, copula_data_np)\n",
    "            clayton_result = f\"Theta: {clayton_theta:.4f}, Log-Lik: {clayton_log_likelihood:.4f}, LRT: {clayton_lrt_stat:.4f}, p: {clayton_p_value:.4f}\"\n",
    "        except Exception as e:\n",
    "            clayton_result = f\"Error: {e}\"\n",
    "\n",
    "        # Lưu kết quả vào danh sách\n",
    "        results.append({\n",
    "            'Pair': f'{col1}-{col2}',\n",
    "            'Frank': frank_result,\n",
    "            'Clayton': clayton_result\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)  \n",
    "print(\"Copula Model Estimation Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, t, kendalltau\n",
    "\n",
    "ranked_data = urc_data.apply(lambda x: rankdata(x) / (len(x) + 1), axis=0)\n",
    "\n",
    "kendall_matrix = np.zeros((len(columns), len(columns)))\n",
    "p_values_matrix = np.zeros((len(columns), len(columns)))\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    for j in range(len(columns)):\n",
    "        if i == j:\n",
    "            kendall_matrix[i, j] = 1\n",
    "            p_values_matrix[i, j] = 0  \n",
    "        else:\n",
    "            tau, p_value = kendalltau(ranked_data[columns[i]], ranked_data[columns[j]])\n",
    "            kendall_matrix[i, j] = tau\n",
    "            p_values_matrix[i, j] = p_value\n",
    "\n",
    "\n",
    "gaussian_correlation_matrix = np.sin(np.pi / 2 * kendall_matrix)\n",
    "\n",
    "df = 5 \n",
    "student_correlation_matrix = np.zeros_like(gaussian_correlation_matrix)\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    for j in range(len(columns)):\n",
    "        if i == j:\n",
    "            student_correlation_matrix[i, j] = 1\n",
    "        else:\n",
    "            student_correlation_matrix[i, j] = gaussian_correlation_matrix[i, j] * (df / (df - 2))\n",
    "\n",
    "results_gaussian = pd.DataFrame(index=columns, columns=columns)\n",
    "results_student = pd.DataFrame(index=columns, columns=columns)\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    for j in range(len(columns)):\n",
    "        if i == j:\n",
    "            results_gaussian.iloc[i, j] = f\"{gaussian_correlation_matrix[i, j]:.4f} [0]\"\n",
    "            results_student.iloc[i, j] = f\"{student_correlation_matrix[i, j]:.4f} [0]\"\n",
    "        else:\n",
    "            results_gaussian.iloc[i, j] = f\"{gaussian_correlation_matrix[i, j]:.4f} [{p_values_matrix[i, j]:.3f}]\"\n",
    "            results_student.iloc[i, j] = f\"{student_correlation_matrix[i, j]:.4f} [{p_values_matrix[i, j]:.3f}]\"\n",
    "\n",
    "print(\"Gaussian Copula Correlation Matrix with P-values:\")\n",
    "print(results_gaussian)\n",
    "\n",
    "print(\"\\nStudent-t Copula Correlation Matrix with P-values:\")\n",
    "print(results_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, t\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Input: Hệ số tương quan từ Copula và lợi nhuận tài sản\n",
    "copula_results = {\n",
    "    \"BTC-Gold\": {\"Frank\": -0.2820, \"Clayton\": -0.0607, \"T-Student\": -0.0819, \"Gaussian\": -0.0492},\n",
    "    \"BTC-VNI\": {\"Frank\": 0.1241, \"Clayton\": 0.0279, \"T-Student\": 0.036, \"Gaussian\": 0.0216},\n",
    "    \"VNI-Gold\": {\"Frank\": 0.1968, \"Clayton\": 0.0447, \"T-Student\": 0.0572, \"Gaussian\": 0.0343}\n",
    "}\n",
    "\n",
    "\n",
    "# Tổ chức lại dữ liệu\n",
    "returns_data = merged_data\n",
    "\n",
    "# Bước 1: Mô phỏng lợi nhuận danh mục đầu tư từ Copula\n",
    "def simulate_portfolio_returns(copula_rho, asset1_returns, asset2_returns, weights, num_simulations=100000):\n",
    "    cov_matrix = np.array([[1, copula_rho], [copula_rho, 1]])\n",
    "    normal_samples = multivariate_normal.rvs(mean=[0, 0], cov=cov_matrix, size=num_simulations)\n",
    "    uniform_samples = norm.cdf(normal_samples)\n",
    "    asset1_simulated = np.quantile(asset1_returns, uniform_samples[:, 1])\n",
    "    asset2_simulated = np.quantile(asset2_returns, uniform_samples[:, 1])\n",
    "    portfolio_returns = weights[0] * asset1_simulated + weights[1] * asset2_simulated\n",
    "    return portfolio_returns\n",
    "\n",
    "# Bước 2: Tính toán VaR\n",
    "def calculate_var(portfolio_returns, confidence_level=0.99):\n",
    "    var = np.percentile(portfolio_returns, (1 - confidence_level) * 100)\n",
    "    return var\n",
    "\n",
    "# Bước 3: Historical Simulation (HS)\n",
    "def historical_var(asset1_returns, asset2_returns, weights, confidence_level=0.99):\n",
    "    portfolio_returns = weights[0] * asset1_returns + weights[1] * asset2_returns\n",
    "    var = np.percentile(portfolio_returns, (1 - confidence_level) * 100)\n",
    "    return var\n",
    "\n",
    "# Bước 4: Variance-Covariance (VC)\n",
    "def variance_covariance_var(asset1_returns, asset2_returns, weights, confidence_level=0.99):\n",
    "    portfolio_mean = weights[0] * np.mean(asset1_returns) + weights[1] * np.mean(asset2_returns)\n",
    "    portfolio_variance = (weights[0]**2 * np.var(asset1_returns) +\n",
    "                          weights[1]**2 * np.var(asset2_returns) +\n",
    "                          2 * weights[0] * weights[1] * np.cov(asset1_returns, asset2_returns)[0, 1])\n",
    "    portfolio_std = np.sqrt(portfolio_variance)\n",
    "    var = portfolio_mean - norm.ppf(confidence_level) * portfolio_std\n",
    "    return var\n",
    "\n",
    "# Tính toán VaR cho từng cặp tài sản và phương pháp\n",
    "confidence_levels = [0.99, 0.95, 0.90]\n",
    "weights = [0.5, 0.5]\n",
    "results = []\n",
    "\n",
    "for pair, copulas in copula_results.items():\n",
    "    asset1, asset2 = pair.split(\"-\")\n",
    "    asset1_returns = returns_data[asset1].values\n",
    "    asset2_returns = returns_data[asset2].values\n",
    "    \n",
    "    for copula, rho in copulas.items():\n",
    "        simulated_returns = simulate_portfolio_returns(rho, asset1_returns, asset2_returns, weights)\n",
    "        for cl in confidence_levels:\n",
    "            results.append({\n",
    "                \"Pair\": pair,\n",
    "                \"Method\": copula,\n",
    "                \"Confidence Level\": f\"VaR({int(cl * 100)}%)\",\n",
    "                \"VaR\": calculate_var(simulated_returns, cl)\n",
    "            })\n",
    "    \n",
    "    for cl in confidence_levels:\n",
    "        results.append({\n",
    "            \"Pair\": pair,\n",
    "            \"Method\": \"HS\",\n",
    "            \"Confidence Level\": f\"VaR({int(cl * 100)}%)\",\n",
    "            \"VaR\": historical_var(asset1_returns, asset2_returns, weights, cl)\n",
    "        })\n",
    "        results.append({\n",
    "            \"Pair\": pair,\n",
    "            \"Method\": \"VC\",\n",
    "            \"Confidence Level\": f\"VaR({int(cl * 100)}%)\",\n",
    "            \"VaR\": variance_covariance_var(asset1_returns, asset2_returns, weights, cl)\n",
    "        })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "pivot_table = results_df.pivot_table(index=[\"Pair\", \"Confidence Level\"], columns=\"Method\", values=\"VaR\")\n",
    "pivot_table = pivot_table.reset_index()\n",
    "print(pivot_table)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Sử dụng pivot_table từ phần tính toán VaR\n",
    "backtesting_data = pivot_table  # Kết quả từ đoạn code trước\n",
    "\n",
    "# Hàm kiểm định VaR\n",
    "def var_backtest(returns_data, backtesting_data, confidence_level, expected_exceedances):\n",
    "    exceedances = returns_data < -backtesting_data  # Kiểm tra vi phạm\n",
    "    num_exceedances = exceedances.sum()  # Số lần vượt mức\n",
    "    n = len(returns_data)  # Tổng số quan sát\n",
    "\n",
    "    # Kiểm định Unconditional Coverage (UC)\n",
    "    p_hat = num_exceedances / n  # Tỉ lệ vi phạm thực tế\n",
    "    if p_hat == 0 or p_hat == 1:\n",
    "        uc_stat = np.inf\n",
    "        uc_p_value = 0.0\n",
    "    else:\n",
    "        uc_stat = -2 * (\n",
    "            (n - num_exceedances) * np.log(1 - confidence_level) +\n",
    "            num_exceedances * np.log(confidence_level) -\n",
    "            (n - num_exceedances) * np.log(1 - p_hat) -\n",
    "            num_exceedances * np.log(p_hat)\n",
    "        )\n",
    "        uc_p_value = 1 - chi2.cdf(uc_stat, 1)\n",
    "\n",
    "    # Kiểm định Independence (IND)\n",
    "    if num_exceedances > 0:\n",
    "        ind_test_result = acorr_ljungbox(exceedances.astype(int), lags=[1], return_df=True)\n",
    "        ind_stat = ind_test_result['lb_stat'].iloc[0]\n",
    "        ind_p_value = ind_test_result['lb_pvalue'].iloc[0]\n",
    "    else:\n",
    "        ind_stat = np.nan\n",
    "        ind_p_value = np.nan\n",
    "\n",
    "    # Kiểm định Conditional Coverage (CC)\n",
    "    if np.isfinite(uc_stat) and np.isfinite(ind_stat):\n",
    "        cc_stat = uc_stat + ind_stat\n",
    "        cc_p_value = 1 - chi2.cdf(cc_stat, 2)\n",
    "    else:\n",
    "        cc_stat = np.nan\n",
    "        cc_p_value = np.nan\n",
    "\n",
    "    # Kết quả kiểm định\n",
    "    test_result = \"AAA\" if uc_p_value > 0.05 and ind_p_value > 0.05 and cc_p_value > 0.05 else \\\n",
    "              \"RAA\" if uc_p_value <= 0.05 and ind_p_value > 0.05 and cc_p_value > 0.05 else \\\n",
    "              \"ARA\" if uc_p_value > 0.05 and ind_p_value <= 0.05 and cc_p_value > 0.05 else \\\n",
    "              \"RRA\" if uc_p_value <= 0.05 and ind_p_value <= 0.05 and cc_p_value > 0.05 else \\\n",
    "              \"RAR\" if uc_p_value <= 0.05 and ind_p_value > 0.05 and cc_p_value <= 0.05 else \\\n",
    "              \"ARR\" if uc_p_value > 0.05 and ind_p_value <= 0.05 and cc_p_value <= 0.05 else \"RRR\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        'num_exceedances': num_exceedances,\n",
    "        'Expected exceedances': expected_exceedances,\n",
    "        'UC Stat': uc_stat,\n",
    "        'UC p-value': uc_p_value,\n",
    "        'IND Stat': ind_stat,\n",
    "        'IND p-value': ind_p_value,\n",
    "        'CC Stat': cc_stat,\n",
    "        'CC p-value': cc_p_value,\n",
    "        'Test Result': test_result\n",
    "    }\n",
    "\n",
    "\n",
    "# Chuẩn bị kiểm định cho từng cặp tài sản, phương pháp, và mức tin cậy\n",
    "confidence_levels = [0.99, 0.95, 0.90]\n",
    "expected_exceedances = {0.99: 1000, 0.95: 5000, 0.90: 10000}  # Số lần vi phạm kỳ vọng dựa trên kích thước mẫu\n",
    "backtest_results = []\n",
    "\n",
    "for index, row in pivot_table.iterrows():\n",
    "    pair = row[\"Pair\"]\n",
    "    confidence_level = float(row[\"Confidence Level\"].replace(\"VaR(\", \"\").replace(\"%)\", \"\")) / 100\n",
    "    for method in [\"HS\", \"VC\", \"Gaussian\", \"T-Student\", \"Clayton\", \"Frank\"]:\n",
    "        # Lấy lợi nhuận thực tế từ returns_data\n",
    "        asset1, asset2 = pair.split(\"-\")\n",
    "        returns = 0.5 * returns_data[asset1] + 0.5 * returns_data[asset2]  #tính lợi nhuận danh mục\n",
    "\n",
    "        # Lấy giá trị VaR từ bảng pivot_table\n",
    "        var_value = row[method]\n",
    "\n",
    "        # Kiểm định VaR\n",
    "        test_results = var_backtest(returns, var_value, confidence_level, expected_exceedances[confidence_level])\n",
    "        backtest_results.append({\n",
    "            \"Pair\": pair,\n",
    "            \"Method\": method,\n",
    "            \"Confidence Level\": row[\"Confidence Level\"],\n",
    "            \"Exceedances\": test_results['num_exceedances'],\n",
    "            \"Expected Exceedances\": test_results['Expected exceedances'],\n",
    "            \"UC Stat\": test_results['UC Stat'],\n",
    "            \"UC p-value\": test_results['UC p-value'],\n",
    "            \"IND Stat\": test_results['IND Stat'],\n",
    "            \"IND p-value\": test_results['IND p-value'],\n",
    "            \"CC Stat\": test_results['CC Stat'],\n",
    "            \"CC p-value\": test_results['CC p-value'],\n",
    "            \"Test Result\": test_results['Test Result']\n",
    "        })\n",
    "\n",
    "backtest_results_df = pd.DataFrame(backtest_results)\n",
    "\n",
    "print(\"VaR Backtesting Results:\")\n",
    "print(backtest_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "def calcAvgSpilloversTable(merged_data, forecast_horizon=10, lag_order=None):\n",
    "\n",
    "\tforecast_horizon = 10 if forecast_horizon is None else forecast_horizon\n",
    "\t\n",
    "\tmodel = VAR(merged_data)\n",
    "\tif lag_order is None:\n",
    "\t\tresults = model.fit(ic='aic')\n",
    "\t\tlag_order = results.k_ar\n",
    "\telse:\n",
    "\t\tresults = model.fit(lag_order)\n",
    "\t\n",
    "\tsigma_u = np.asarray(results.sigma_u)\n",
    "\tsd_u = np.sqrt(np.diag(sigma_u))\n",
    "\t\n",
    "\tfevd = results.fevd(forecast_horizon, sigma_u/sd_u)\n",
    "\tfe = fevd.decomp[:, -1, :]\n",
    "\tfevd = (fe / fe.sum(1)[:, None] * 100)\n",
    "\n",
    "\tcont_incl = fevd.sum(0)\n",
    "\tcont_to = fevd.sum(0) - np.diag(fevd)\n",
    "\tcont_from  = fevd.sum(1) - np.diag(fevd)\n",
    "\tspillover_index = 100 * cont_to.sum() / cont_incl.sum()\n",
    "\n",
    "\tnames = model.endog_names\n",
    "\tspilloversTable = pd.DataFrame(fevd, columns=names).set_index([names])\n",
    "\tspilloversTable.loc['Cont_To'] = cont_to\n",
    "\tspilloversTable.loc['Cont_Incl'] = cont_incl\n",
    "\tspilloversTable = pd.concat([spilloversTable, pd.DataFrame(cont_from, columns=['Cont_From']).set_index([names])], axis=1)\n",
    "\tspilloversTable = pd.concat([spilloversTable, pd.DataFrame(cont_to - cont_from, columns=['Cont_Net']).set_index([names])], axis=1)\n",
    "\tspilloversTable.loc['Cont_To', 'Cont_From'] = cont_to.sum()\n",
    "\tspilloversTable.loc['Cont_Incl', 'Cont_From'] = cont_incl.sum()\n",
    "\tspilloversTable.loc['Cont_Incl', 'Cont_Net'] = spillover_index\n",
    "\n",
    "\treturn spilloversTable, lag_order, forecast_horizon\n",
    "\n",
    "spilloversTable, lag_order, forecast_horizon = calcAvgSpilloversTable(merged_data)\n",
    "print(spilloversTable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
